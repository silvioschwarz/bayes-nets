% Chapter 2

\chapter{Learning Bayesian Networks} % Main chapter title

\label{Chapter2} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter 2. \emph{Learning}} % This is for the header on each page - perhaps a shortened title
Bayesian networks are directed acyclic graphs (DAG). They are representations of the dependence structure among a set of random variables. The conditional dependencies between the random variables are visualized by directed edges and the random variables them self are the nodes of the network. Through the directed edges it is possibly to identify a hierarchy. The source of a directed edge is called a parent of the receiving node and the receiver is called a child of the source node. Besides the dependence structure each node in the Bayesian Network carries a probability distribution conditional on their parents. In the case of discrete distributions this becomes a probability table.\\
The advantage of using a Bayesian Network is that it represents the joint probability over all the random variables considered. With that it is possible to answer conditional queries through the laws of probability theory. Through the structure of the network it is possible to factorize the joint probability distribution leading to less parameters to estimate and a more efficient computation than considering all possible combinations without losing anything. For the task of the evaluation of natural hazard, and in this specific case of ground motion, Bayesian Networks provide a way to compute the full distribution of the target variable and are thus an efficient and consistent way of dealing with uncertainty. 

%----------------------------------------------------------------------------------------

\section{Building Bayesian Networks}
The task for this assignment is to build different Bayesian Networks, use synthetic data to learn the corresponding parameters and, to evaluate their performance of predicting on the target value.\\
The data consists of a set of six variables commonly used for defining ground motion models and the prediction of ground motion values such as PGA, PSA or macroseismic intensity. Each of the variables has been sampled from a distribution according to Table~\ref{tab:1} and the stochastic model of Boore     \citep{boore2003} was used to compute the corresponding values of PGA which is included in the data as log(PGA) since its values span several orders of magnitude and it is assumed that PGA follows a log-normal distribution, hence, the logarithm of it is a gaussian distribution. In total the dataset comprises 10000 "observations".
\vspace{1.5cm}


\begin{table}[h]
\begin{tabular}{ p{3cm}p{5cm}p{5cm}  }
\hline
 $X_i$ & Description & Distribution$_{[range]}$\\
 \hline 
 \hline
 \multicolumn{3}{c}{Variables} \\
 \hline
 M   & Moment Magnitude    &$\mathcal{U}_{[5,7.5]}$\\
 R&   Distance to Source  & Exp$_{[1 km, 200km]}$\\
 SD & Stress drop & Exp$_{[0 bar, 500bar]}$\\
 $Q_0$    &Attenuation of seismic waves in deep strata &  Exp$_{[0 s^{-1}, 5000 s^{-1}]}$\\
 $\kappa_0$&   Attenuation of seismic waves close to the surface  &  Exp$_{[0 s, 0.1 s]}$\\
 $V_s30$& Average shear wave velocity in the upper 30m  & $\mathcal{U}_{[600 m s^{-1}, 2800 m s^{-1}]}$\\
 \hline
 \multicolumn{3}{c}{Ground Motion Variable} \\
 \hline
 $\log PGA$& logarithm of peak horizontal ground acceleration  & synthetic calculated through the stochastic model of Boore \citep{boore2003}\\
 \hline
\end{tabular}
\caption[Variables and distributions]{Overview over the variables used and their according distributions.}
\label{tab:1}
\end{table}

\newpage
There are different methods of choosing the structure of a Bayesian Network. The easiest and simplest way is to construct a naive Bayes network (Figure~\ref{fig:naive}). This means that the target value connects to all explanatory values and there are no other connections. This is "naive" as it makes the assumption that all explanatory variables are independent from each other. Thus, the joint distribution factorizes simply to the following product:
\begin{center}
\small
P(PGA, SD, MAG, DIST, $Q_0, \kappa_0, V_s30$) = P(PGA)* P(SD$\mid$PGA)* \\P(MAG$\mid$PGA)*P(DIST$\mid$PGA)* P($Q_0\mid$PGA)* P($\kappa_0\mid$PGA)* P($V_s30\mid$PGA)\\
\normalsize
\end{center}
Considering the naive Bayes independence assumption is quite a harsh cut since it is the simplest model you can create by using all variables. Nevertheless it is a attractive choice because it need very few parameters to be estimated and it shows a reasonable performance in real-world applications such as Email spam filter. It is also a good candidate as a starting point because it is questionable to build more complex models that in the end perform equally well or even worse.   

\begin{figure}[!htpb]
	\centering
		\includegraphics[scale=0.45]{Figures/naive.png}
		\rule{35em}{0.5pt}
	\caption[Naive Bayes Network]{A naive Bayes Network of the variables.}
	\label{fig:naive}
\end{figure}
\newpage

Another way of setting up the structure of a Bayesian Network is to rely on expert judgment to define the dependencies between the variables. This is called a causal network since one tries to capture the causal relationships in choosing the dependencies. For the case of predicting PGA from a set of explanatory variables the following causal network(Figure~\ref{fig:causal}) can be reasoned. \\

\begin{figure}[!h]
	\centering
		\includegraphics[scale=0.45]{Figures/causal.png}
		\rule{35em}{0.5pt}
	\caption[Causal Network]{A causal network representing the beliefs in dependency based on expert knowledge.}
	\label{fig:causal}
\end{figure}

One can argue that the attenuation behavior in deep layer (Q0) is independent from the one in shallow layers (kappa) because this difference reflects the varying materials and environment conditions. Nevertheless, since both are material properties they can influence the shear wave velocity in the first 30m ($V_s30$). One can imagine??.  The distance from the source (DIST) doesn't seem dependent on any other explanatory variable because one can imagine having the same earthquake but choosing a different location on the earth's surface. Moment magnitude (MAG) and stress drop (SD) are dependent since they both refer to the energy that is released during an earthquake. Since the moment magnitude is proportional to the ruptured area the same value can be achieved by a wide range of possible combinations in the ruptures' width and length. This is the reason why it is dependent on the stress drop.\\
One could also have had a look at the data \ref{AppendixA}. Clearly PGA depends on all of the variables but it is also possible to see trend between distance and stress drop, Q0 and, $\kappa_0$ which all seem to decrease exponentially with the distance. But a closer look at the relationship between distance and magnitude shows that there are more datapoints at close distances than at far away points. In the case where one doesn't know the underlying distributions it is always a good idea to consult histograms of the maginal distribution of the variables, too. In this case it is known that stress drop, Q0, $\kappa_0$ and, the distance are all sampled from exponential distributions. That means that the trend in the visualization isn't really there it's just that the dependent variable in the plot is also sampled from an exponential distribution o there are less datapoints at larger values. This can actually been seen in the scatterplots as there are datapoints at large values over the entire range of the independet variable. There are just fewer. This little excourse shows some of the dangers of looking at the data before learning models. One is prone to find patterns, often ones that aren't even there. And for a multidimensional dataset such like this two-dimensional representations always loose some of the information. Sometimes this can't even be avoided by looking at the marginals. On another point, in machine learning there exist the notion of "Data snooping"
\begin{quote}
"If a data set has affected any step in the learning process, its ability to assess the outcome has been compromised. \citep{LearningFromData}"
\end{quote}
From a Bayesian point of view one can say that looking at the data to decide any further steps in the analyses is in itself a step of learning. One goes from a state of ignorance where all hypotheses (in this case the structure of the Bayesian net) have equal probability over to excluding certain hypotheses that don't seem to capture the relationships one has seen. This is essential starting with a uniform distribution as a prior to setting some of the probabilities to zero. The danger is that the model is overfitted and so performs poorly on unseen data. In a Bayesian framework the prior can be thought of as a regularization parameter that guards against overfitting. But for this mechanism to work it is best to have some sort of naive??? prior and not to "fit" the prior on data to have a superb in-sample-error but poor out-of-sample performance.\\
Another way to set up causal networks is to consult literatur about the topic, In the case of Bayesian Networks for ground motion prediction sources could be \citep{kuehn2010} or \citep{Vogel2014}.\\

A third way of defining the structure of a Bayesian network is to learn it from the data itself. Often, not all dependencies between the variables are known and as was seen in causal networks, human domain knowledge can also be misleading and sometimes a strong assumption as it limits the hypothesis space of possible networks vastly. So it is a natural extension to ask whether there are principled ways in the framework of Bayesian networks to let also the structure come from the data. In a sense, according to the Bayesian paradigm, the structure of a network becomes a random variable, too and the task is to jointly estimate the parameters and the structure from the data. In the scope of this paper the constraint-based Grow-Shrink algorithm~\citep{margaritis2003} and the score-based hill-climber are explored.\\
Constraint-based algorithms perform independence tests between the random variables and then set up a network according to the found independencies. The task is therefore one of finding the best minimal I-map. An I-map or independence map is a graph whose independence statements hold for the probability distribution one tries to model. In the case where the graph captures all independence statements this is a perfect I-map. A minimal I-map is graph that is rendered not an I-map anymore by the removal of one edge. This is an important definition because the complete graph over a set of random variables is also an I-map but does not reveal any independencies and therefore carries parameters that are redundant.In practice one doe not find one single best minimal I-map but a class of graphs that carry the same independence statements and are therefore called I-equivalent~\citep{koller2009}. 
The Grow-Shrink algorithm tries to construct the structure of a network by finding the Markov Blankets of the variables. A Markov Blanket of one variable is a set of variables that renders that variable to be d-separated from all other variables. That means that knowing the state of any variable that is not in the Markov Blanket has no effect on knowing the state of the variable in interest. One could say that the Markov Blanket "shields" a variable from the influence of all other variables. Graphically it is the set of parents, children and parents of the children of the variable in interest~\citep{koller2009}. In the Growing phase of the Grow-Shrink algorithm independence tests between variables are performed which are the basis to decide if a variable should be included in the Markov blanket. These test occur given the state of the Markov Blanket. Depending on the initial ordering of the variables this can lead to include redundant variables in the Markov Blanket which are subsequently removed by the independence test of the Shrinking phase~\citep{margaritis2003}. For learning the structure of a Bayesian network according to the Grow-Shrink algorithm the mutual information (Equation~\ref{eqn:mutual}) is used as an independence test.

\begin{equation}
I(X;Y) = \sum_{y \in Y} \sum_{x \in X} 
                 p(x,y) \log{ \left(\frac{p(x,y)}{p(x)\,p(y)}
                              \right) }, \,\! 
\label{eqn:mutual}
\end{equation}

 It estimates the dependence between two variables by comparing the joint distribution to the product of the marginal distributions, since in the case of independence the joint distribution factorizes to the product of the marginal distribution. From a Venn-diagram point of view it calculates the area shared by two distribution relative to the total area of the distributions.\\
The learned network is visualized in Fig.\ref{fig:gs}. It is interesting to see that there are no direct dependencies between the explanatory variables. Even more the variable $V_s30$ is completely ignored. By comparing this result to work of~\citep{Vogel2014}which uses a similar data set( ;) ) one can find that this seems to be a consistent result when learning the structure of a Bayesian network for ground motion prediction from data. One causal reason might be that $V_s30$ is merely a proxy in quantifying the capability of the soil to amplify the amplitudes of seismic waves. One should keep in mind that the data was generated by the stochastic model of Boore~\citep{boore2003} and that a number of samples is not the full distribution.\\
\begin{figure}[htbp]
	\centering
		\includegraphics[scale=0.5]{Figures/gs_one.png}
		\rule{35em}{0.5pt}
	\caption[Contraint-based Grow-Shrink Network]{gs}
	\label{fig:gs}
\end{figure}

Score-based algorithms view the problem of finding the structure of a Bayesian network from an optimization point of view. In contrast to the constraint-based algorithms, score-based ones do not try to construct the structure from information about single connections between variables but take the network as a whole, compute a score that measures how well the current structure fit the data and, try to find the network that maximizes that score. Consequently, score-based algorithm pose a search problem in the space of possible network structures. Depending on the number of variables and the underlying probability distribution in most cases this is a NP-hard problem and requires some approximation techniques~\citep{koller2009}. \\
A Hill-climber can be thought of as the opposite of gradient descent since it tries to maximize a predefined score, most commonly a likelihood measure that estimates the probability of the data been generated by the given structure, in contrast to minimizing an error term. For the construction of a Bayesian network using the hill-climber algorithm a score consisting of the is the maximized likelihood L (Equation \ref{eqn:likelihood}) that gives the probability of the data being generated by the graph G and the parameters $\theta$ and the Bayesian Information Criterion (BIC) (Equation~\ref{eqn:BIC}~\citep{schwarz}) as a regularization term consisting of the number of free parameters $k$ and the size of the data $n$ is used.

\begin{equation}
L = \operatorname*{arg\,max}_\theta P(x\mid \theta, G)
\label{eqn:likelihood}
\end{equation}

\begin{equation}
BIC = -2*ln L + k* ln(n)
\label{eqn:BIC}
\end{equation}


\begin{figure}[htbp]%
	\centering
		\includegraphics[scale=0.5]{Figures/hc_one.png}
		\rule{35em}{0.5pt}
	\caption[Score-based Hill-Climber Network]{hc}
	\label{fig:hc}
\end{figure}

there is a third. bayes model averaging. create ensemble of nets and average the predictions.
The learned structure networks look pretty similar. Compare to \citep{Vogel2014} because they are from similar data. Actually a series of networks was learned: \ref{AppendixA} \ref{AppendixB}

%----------------------------------------------------------------------------------------

\section{Parameter Learning}

bayesian parameter estimation
%----------------------------------------------------------------------------------------

