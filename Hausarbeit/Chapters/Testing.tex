% Chapter 1

\chapter{Testing} % Main chapter title

\label{Chapter3} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter 3. \emph{Testing}} % This is for the header on each page - perhaps a shortened title

Testing the outcomes of a model is an integral part of the modeling since it gives an estimate on the expected performance in future scenarios. For doing so, many different measures and methodologies have been developed, all targeting a different part of the question what a "good" model should be. There are measures like the mean squared error that quantify the difference between the observed data points and the values predicted from the model. These can be thought of as in-sample error metrics because the data to construct the model is also used to estimate the error. Intuitively, this seems like a good idea because it gives an easy to use metric that establishes an link between the model and the reality. But focusing too much on getting a low in-sample-error can cause a phenomenon called "overfitting" where the model is so much adjusted to the present data that it performs poorly on unseen data. This can even lead to the conviction that some of the data has to be excluded because the worsen the fit.\\
This is the difference between function approximation and learning a model. In function approximation the goal is to estimate the parameters of a model so that the final function matches the given data the closest. In learning a model the underlying dependencies are usually not known and the data only represent a subset of the whole range of possible values, often including some noise. Hence, the task in learning a model is to match the model complexity to the data resources~\citep{LearningFromData}. Particular in a scenario of estimating natural hazard it would be desirable to have a measure that can quantify the model's ability for forecasting.\\\\
In the scope of this paper the mode, median and mean are used asq point estimates and the mean squared error is computed. Additionally, the log-likelihood of the observed PGA values.

%----------------------------------------------------------------------------------------

\section{Validation}
One common approach to get a handle of a models' prediction performance is to divide the data into a learning set and a validation set. Then the learning data is used to estimate the parameters and the models performance is tested on the test set. This has the effect of simulating unseen data since the test data has not been used for learning the models parameters.

\begin{table}[h]
\centering
\begin{tabular}{ l l l l l  }
 \hline
  & \multicolumn{4}{c}{error measure} \\
 \hline
 net & mean & median & mode & probability\\
 \hline
 causal   & 4.816    & 4.933  & 9.739   &-1.772\\
 naive   & 1.488      & 1.801 & 1.937  &-1.35\\
 hill-climber   & 1.068      & 1.339 & 1.503  &-1.016\\
 grow-shrink   & 0.844     & 1.124  & 1.149 &-0.874\\
\end{tabular}
\caption[Validation Error]{error measures}
\label{tab:2}
\end{table}
%----------------------------------------------------------------------------------------

\section{Crossvalidation}
Dividing the available data into a learning and a validation set make the compromise to spend a certain portion of the data to validation and to not use this data in the estimation of the models parameters. One has to find a balance between not setting aside too much data for the validation and consequently learning a poor model from few data points and not using enough data for validation so that the estimated future performance is not well defined. The method of crossvalidation addresses this dilemma by not learning one model and testing on one data set but performing this procedure several times and average the results. Therefore, the data is divided into so called folds.

\begin{table}[h]
\centering
\begin{tabular}{ l l l l l  }
 \hline
  & \multicolumn{4}{c}{error measure} \\
 \hline
 net & mean & median & mode & probability\\
 \hline
 causal   & 4.904    & 5.066  & 9.421   &-1.764\\
 naive   & 1.482      & 1.703 & 1.762  &-1.342\\
 hill-climber   & 1.041      & 1.488 & 1.503  &-1.017\\
 grow-shrink   & 0.823     & 1.202  & 1.149 &-0.875\\
 
\end{tabular}
\caption[Crossvalidation Error]{error measures}
\label{tab:3}
\end{table}
%----------------------------------------------------------------------------------------

\section{Bias and Variance Decomposition}

\begin{table}[h]
\centering
\begin{tabular}{ l l l l }
 \hline
 & \multicolumn{3}{c}{error measure} \\
 \hline
 net & mean & median & mode\\
 \hline
 causal & 4.876 & 5.007 & 9.233\\
 naive & 1.472 & 1.682 & 1.709\\
 hill-climber & 1.029 & 1.341 & 1.478\\
 grow-shrink & 0.786 & 1.115 & 1.152\\
\end{tabular}
\caption[Bias]{error measures}
\label{tab:4}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{ l l l l }
 \hline
  & \multicolumn{3}{c}{error measure} \\
 \hline
 net & mean & median & mode\\
 \hline
 causal & 0.01& 0.185 & 1.661\\
 naive & 0.0091 & 0.136 & 0.546\\
 hill-climber & 0.0032 & 0.0694 & 0.128\\
 grow-shrink & 0.003 & 0.0622& 0.092\\
\end{tabular}
\caption[Variance]{error measures}
\label{tab:5}
\end{table}